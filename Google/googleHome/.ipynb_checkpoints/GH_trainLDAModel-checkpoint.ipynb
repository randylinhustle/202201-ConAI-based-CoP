{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, numpy as np, pandas as pd\n",
    "import tqdm\n",
    "import glob\n",
    "from cleantext import clean\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['https_www','seems_like','do','not','imgur','tkg','https','http','could','www','com','ever','doesnt_seem',\n",
    "                  'xxxx','else','would','also','ea','&amp','#x200B','oh','etc','yeah','nan','however','even','dont_know','sa',\n",
    "                  \"looks_like\",'especially','may','sounds_like'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59823, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_link</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>num_of_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>flair</th>\n",
       "      <th>comment_msg</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4m130w</td>\n",
       "      <td>No surprise, Google Home is based on Chromecas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/4...</td>\n",
       "      <td>seekweb</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-01 20:53:18</td>\n",
       "      <td>4</td>\n",
       "      <td>/r/googlehome/comments/4m130w/no_surprise_goog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"Anybody actually use this sub, yet?  Guess i...</td>\n",
       "      <td>No surprise, Google Home is based on Chromecas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title selftext  \\\n",
       "0  4m130w  No surprise, Google Home is based on Chromecas...      NaN   \n",
       "\n",
       "                                           full_link   author  score  \\\n",
       "0  https://www.reddit.com/r/googlehome/comments/4...  seekweb      2   \n",
       "\n",
       "          publish_date  num_of_comments  \\\n",
       "0  2016-06-01 20:53:18                4   \n",
       "\n",
       "                                           permalink flair  \\\n",
       "0  /r/googlehome/comments/4m130w/no_surprise_goog...   NaN   \n",
       "\n",
       "                                         comment_msg  \\\n",
       "0  [\"Anybody actually use this sub, yet?  Guess i...   \n",
       "\n",
       "                                             content  \n",
       "0  No surprise, Google Home is based on Chromecas...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoadDataset\n",
    "df=pd.read_csv('googlehome_merged.csv')\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_link</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>num_of_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>flair</th>\n",
       "      <th>comment_msg</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4m130w</td>\n",
       "      <td>No surprise, Google Home is based on Chromecas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/4...</td>\n",
       "      <td>seekweb</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-01 20:53:18</td>\n",
       "      <td>4</td>\n",
       "      <td>/r/googlehome/comments/4m130w/no_surprise_goog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"Anybody actually use this sub, yet?  Guess i...</td>\n",
       "      <td>no surprise google home is based on chromecast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4mrqc7</td>\n",
       "      <td>IoT Is Getting Better With Google Home - An Am...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/4...</td>\n",
       "      <td>K2Bsolutions</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-06 15:04:12</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/googlehome/comments/4mrqc7/iot_is_getting_b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>iot is getting better with google home an amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4zjsqh</td>\n",
       "      <td>Any update? Is thing happening?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/4...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-08-26 01:45:35</td>\n",
       "      <td>3</td>\n",
       "      <td>/r/googlehome/comments/4zjsqh/any_update_is_th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"I've been wondering that myself. I can't fin...</td>\n",
       "      <td>any update is thing happening ive been wonderi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50iyx8</td>\n",
       "      <td>Google is taking dozens of Nest engineers to w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/5...</td>\n",
       "      <td>my_bday_is_tomorrow</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-09-01 03:19:28</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/googlehome/comments/50iyx8/google_is_taking...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>google is taking dozens of nest engineers to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5469ci</td>\n",
       "      <td>Android Police: Google Home will cost $129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/5...</td>\n",
       "      <td>chopper_woot_woot</td>\n",
       "      <td>12</td>\n",
       "      <td>2016-09-24 02:41:27</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/googlehome/comments/5469ci/android_police_g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>android police google home will cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59818</td>\n",
       "      <td>np4n38</td>\n",
       "      <td>Just a little fun for the young kids - Use a t...</td>\n",
       "      <td>I have speakers all throughout my house, so no...</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/n...</td>\n",
       "      <td>mywerkaccount</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-31 22:27:27</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/googlehome/comments/np4n38/just_a_little_fu...</td>\n",
       "      <td>Tips</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>just a little fun for the young kids use a tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59819</td>\n",
       "      <td>np4xyn</td>\n",
       "      <td>I'm planning to get google nest mini (2nd Gen)...</td>\n",
       "      <td>i just wanna try a smart home speaker. this is...</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/n...</td>\n",
       "      <td>Embarrassed-Ad8685</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-31 22:41:19</td>\n",
       "      <td>7</td>\n",
       "      <td>/r/googlehome/comments/np4xyn/im_planning_to_g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Yep, you should be fine, even if they should...</td>\n",
       "      <td>im planning to get google nest mini nd gen is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59820</td>\n",
       "      <td>np4zu4</td>\n",
       "      <td>Workday routine skipping all steps but last one</td>\n",
       "      <td>For the past couple weeks, my morning routine ...</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/n...</td>\n",
       "      <td>LingonberryNarrow755</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-31 22:43:42</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/googlehome/comments/np4zu4/workday_routine_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>workday routine skipping all steps but last on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59821</td>\n",
       "      <td>np5lzm</td>\n",
       "      <td>Forsage Busd Review 2021: Legit Or Scam? Read ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/n...</td>\n",
       "      <td>Naijabizplug</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-31 23:10:57</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/googlehome/comments/np5lzm/forsage_busd_rev...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>forsage busd review legit or scam read how it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59822</td>\n",
       "      <td>np5s8s</td>\n",
       "      <td>Google Home 10 day Weather forecast anyone els...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/n...</td>\n",
       "      <td>Newwales2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-31 23:18:38</td>\n",
       "      <td>53</td>\n",
       "      <td>/r/googlehome/comments/np5s8s/google_home_10_d...</td>\n",
       "      <td>Assistant Activation Trigger Warning!</td>\n",
       "      <td>['Google Assistant gave that forecast like if ...</td>\n",
       "      <td>google home day weather forecast anyone else f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59823 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                              title  \\\n",
       "0      4m130w  No surprise, Google Home is based on Chromecas...   \n",
       "1      4mrqc7  IoT Is Getting Better With Google Home - An Am...   \n",
       "2      4zjsqh                    Any update? Is thing happening?   \n",
       "3      50iyx8  Google is taking dozens of Nest engineers to w...   \n",
       "4      5469ci         Android Police: Google Home will cost $129   \n",
       "...       ...                                                ...   \n",
       "59818  np4n38  Just a little fun for the young kids - Use a t...   \n",
       "59819  np4xyn  I'm planning to get google nest mini (2nd Gen)...   \n",
       "59820  np4zu4    Workday routine skipping all steps but last one   \n",
       "59821  np5lzm  Forsage Busd Review 2021: Legit Or Scam? Read ...   \n",
       "59822  np5s8s  Google Home 10 day Weather forecast anyone els...   \n",
       "\n",
       "                                                selftext  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "59818  I have speakers all throughout my house, so no...   \n",
       "59819  i just wanna try a smart home speaker. this is...   \n",
       "59820  For the past couple weeks, my morning routine ...   \n",
       "59821                                                NaN   \n",
       "59822                                                NaN   \n",
       "\n",
       "                                               full_link  \\\n",
       "0      https://www.reddit.com/r/googlehome/comments/4...   \n",
       "1      https://www.reddit.com/r/googlehome/comments/4...   \n",
       "2      https://www.reddit.com/r/googlehome/comments/4...   \n",
       "3      https://www.reddit.com/r/googlehome/comments/5...   \n",
       "4      https://www.reddit.com/r/googlehome/comments/5...   \n",
       "...                                                  ...   \n",
       "59818  https://www.reddit.com/r/googlehome/comments/n...   \n",
       "59819  https://www.reddit.com/r/googlehome/comments/n...   \n",
       "59820  https://www.reddit.com/r/googlehome/comments/n...   \n",
       "59821  https://www.reddit.com/r/googlehome/comments/n...   \n",
       "59822  https://www.reddit.com/r/googlehome/comments/n...   \n",
       "\n",
       "                     author  score         publish_date  num_of_comments  \\\n",
       "0                   seekweb      2  2016-06-01 20:53:18                4   \n",
       "1              K2Bsolutions      1  2016-06-06 15:04:12                0   \n",
       "2                 [deleted]      3  2016-08-26 01:45:35                3   \n",
       "3       my_bday_is_tomorrow      2  2016-09-01 03:19:28                0   \n",
       "4         chopper_woot_woot     12  2016-09-24 02:41:27                0   \n",
       "...                     ...    ...                  ...              ...   \n",
       "59818         mywerkaccount      1  2021-05-31 22:27:27                1   \n",
       "59819    Embarrassed-Ad8685      1  2021-05-31 22:41:19                7   \n",
       "59820  LingonberryNarrow755      1  2021-05-31 22:43:42                0   \n",
       "59821          Naijabizplug      1  2021-05-31 23:10:57                0   \n",
       "59822             Newwales2      1  2021-05-31 23:18:38               53   \n",
       "\n",
       "                                               permalink  \\\n",
       "0      /r/googlehome/comments/4m130w/no_surprise_goog...   \n",
       "1      /r/googlehome/comments/4mrqc7/iot_is_getting_b...   \n",
       "2      /r/googlehome/comments/4zjsqh/any_update_is_th...   \n",
       "3      /r/googlehome/comments/50iyx8/google_is_taking...   \n",
       "4      /r/googlehome/comments/5469ci/android_police_g...   \n",
       "...                                                  ...   \n",
       "59818  /r/googlehome/comments/np4n38/just_a_little_fu...   \n",
       "59819  /r/googlehome/comments/np4xyn/im_planning_to_g...   \n",
       "59820  /r/googlehome/comments/np4zu4/workday_routine_...   \n",
       "59821  /r/googlehome/comments/np5lzm/forsage_busd_rev...   \n",
       "59822  /r/googlehome/comments/np5s8s/google_home_10_d...   \n",
       "\n",
       "                                       flair  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "...                                      ...   \n",
       "59818                                   Tips   \n",
       "59819                                    NaN   \n",
       "59820                                    NaN   \n",
       "59821                                    NaN   \n",
       "59822  Assistant Activation Trigger Warning!   \n",
       "\n",
       "                                             comment_msg  \\\n",
       "0      [\"Anybody actually use this sub, yet?  Guess i...   \n",
       "1                                                ['nan']   \n",
       "2      [\"I've been wondering that myself. I can't fin...   \n",
       "3                                                ['nan']   \n",
       "4                                                ['nan']   \n",
       "...                                                  ...   \n",
       "59818                                            ['nan']   \n",
       "59819  ['Yep, you should be fine, even if they should...   \n",
       "59820                                            ['nan']   \n",
       "59821                                            ['nan']   \n",
       "59822  ['Google Assistant gave that forecast like if ...   \n",
       "\n",
       "                                                 content  \n",
       "0      no surprise google home is based on chromecast...  \n",
       "1      iot is getting better with google home an amaz...  \n",
       "2      any update is thing happening ive been wonderi...  \n",
       "3      google is taking dozens of nest engineers to w...  \n",
       "4                  android police google home will cost   \n",
       "...                                                  ...  \n",
       "59818  just a little fun for the young kids use a tex...  \n",
       "59819  im planning to get google nest mini nd gen is ...  \n",
       "59820  workday routine skipping all steps but last on...  \n",
       "59821  forsage busd review legit or scam read how it ...  \n",
       "59822  google home day weather forecast anyone else f...  \n",
       "\n",
       "[59823 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
    "\n",
    "def preprocess_tweet(row):\n",
    "    text = row['content']\n",
    "    text = text.replace('r/','')\n",
    "    text = p.clean(text)\n",
    "    text = clean(text,     \n",
    "                 fix_unicode=True,              # fix various unicode errors\n",
    "                 to_ascii=True,                 # transliterate to closest ASCII representation\n",
    "                 lower=True,                    # lowercase text\n",
    "                 no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "                 no_urls=True,                  # replace all URLs with a special token\n",
    "                 no_emails=True,                # replace all email addresses with a special token\n",
    "                 no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "                 no_numbers=True,               # replace all numbers with a special token\n",
    "                 no_digits=True,                # replace all digits with a special token\n",
    "                 no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "                 no_punct=True,                 # remove punctuations\n",
    "                 lang=\"en\",                     # set to 'de' for German special handling\n",
    "                 replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "                 replace_with_url=\"\",\n",
    "                 replace_with_email=\"\",\n",
    "                 replace_with_phone_number=\"\",\n",
    "                 replace_with_number=\"\",\n",
    "                 replace_with_digit=\"\",\n",
    "                 replace_with_currency_symbol=\"\"\n",
    "                )\n",
    "    text = text.replace('amp','')\n",
    "    text = text.replace('nan','')\n",
    "    return text\n",
    "\n",
    "df['content'] = df.apply(preprocess_tweet, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no', 'surprise', 'google', 'home', 'is', 'based', 'on', 'chromecast', 'not', 'android', 'anybody', 'actually', 'use', 'this', 'sub', 'yet', 'guess', 'it', 'kinda', 'hard', 'to', 'when', 'there', 'isnt', 'product', 'yetchromecast', 'is', 'based', 'more', 'on', 'android', 'than', 'on', 'chromeos', 'so', 'this', 'doesnt', 'mean', 'much']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub(r'http\\S+', '', sent) # remove http\n",
    "        sent = re.sub(r'https\\S+', '', sent) # remove https\n",
    "        sent = re.sub('<[^>]+>', '', sent) # remove HTML tags\n",
    "        sent = re.sub('<[^<]+?>', '', sent)\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(r'[^\\w\\s]','',sent) # remove punctuations\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), min_len=2, deacc=True) \n",
    "        \n",
    "        yield(sent)  \n",
    "\n",
    "# # Convert to list\n",
    "data = df.content.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['surprise', 'google_home', 'base', 'chromecast', 'android', 'actually_use', 'sub', 'yet', 'guess', 'kinda_hard', 'product', 'yetchromecast', 'base', 'android', 'chromeos', 'doesnt_mean', 'much']]\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=1,delimiter='_') # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=1, delimiter='_')  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Tag   Meaning                English Examples\n",
    "# ADJ   adjective              new, good, high, special, big, local\n",
    "# ADP   adposition             on, of, at, with, by, into, under\n",
    "# ADV   adverb                 really, already, still, early, now\n",
    "# CONJ  conjunction            and, or, but, if, while, although\n",
    "# DET   determiner, article    the, a, some, most, every, no, which\n",
    "# NOUN  noun                   year, home, costs, time, Africa\n",
    "# NUM   numeral                twenty-four, fourth, 1991, 14:24\n",
    "# PRT   particle               at, on, out, over per, that, up, with\n",
    "# PRON  pronoun                he, their, her, its, my, I, us\n",
    "# VERB  verb                   is, say, told, given, playing, would\n",
    "# .     punctuation marks      . , ; !\n",
    "# X     other                  ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "# def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "def process_words(texts, stop_words=stop_words, disallowed_postags=['ADP', 'CONJ', 'DET', 'NUM', 'PRT','PRON','.','X']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ not in disallowed_postags])\n",
    "#         texts_out.append([token.lemma_ for token in doc])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), max_len=20) if word not in stop_words] for doc in texts_out] \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "print(data_ready[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 152005\n",
      "Number of unique words after removing rare and common words: 1406\n",
      "Number of documents: 59823\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = Dictionary(data_ready)\n",
    "print('Number of unique words in initital documents:', len(id2word))\n",
    "\n",
    "# Filter out words that occur less than 0.5% documents, or more than 20% of the documents.\n",
    "id2word.filter_extremes(no_below = (round(((len(data_ready))*0.005))), no_above = 0.99)\n",
    "print('Number of unique words after removing rare and common words:', len(id2word))\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.save(\"corpus_dict/dict\")\n",
    "corpora.MmCorpus.serialize(\"corpus_dict/corpus\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_link</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>num_of_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>flair</th>\n",
       "      <th>comment_msg</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4m130w</td>\n",
       "      <td>No surprise, Google Home is based on Chromecas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/googlehome/comments/4...</td>\n",
       "      <td>seekweb</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-01 20:53:18</td>\n",
       "      <td>4</td>\n",
       "      <td>/r/googlehome/comments/4m130w/no_surprise_goog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"Anybody actually use this sub, yet?  Guess i...</td>\n",
       "      <td>no surprise google home is based on chromecast...</td>\n",
       "      <td>[android, base, chromecast, google_home, guess...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title selftext  \\\n",
       "0  4m130w  No surprise, Google Home is based on Chromecas...      NaN   \n",
       "\n",
       "                                           full_link   author  score  \\\n",
       "0  https://www.reddit.com/r/googlehome/comments/4...  seekweb      2   \n",
       "\n",
       "          publish_date  num_of_comments  \\\n",
       "0  2016-06-01 20:53:18                4   \n",
       "\n",
       "                                           permalink flair  \\\n",
       "0  /r/googlehome/comments/4m130w/no_surprise_goog...   NaN   \n",
       "\n",
       "                                         comment_msg  \\\n",
       "0  [\"Anybody actually use this sub, yet?  Guess i...   \n",
       "\n",
       "                                             content  \\\n",
       "0  no surprise google home is based on chromecast...   \n",
       "\n",
       "                                              tokenz  \n",
       "0  [android, base, chromecast, google_home, guess...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenz'] = [[(id2word[id]) for id, freq in cp] for cp in corpus[:]]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('1_df_content_tokenz.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('android', 2), ('base', 2), ('chromecast', 1), ('google_home', 1), ('guess', 1), ('much', 1), ('product', 1), ('sub', 1), ('surprise', 1), ('yet', 1)]]\n"
     ]
    }
   ],
   "source": [
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tf-idf\n",
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# # Create Dictionary\n",
    "# from gensim import models\n",
    "\n",
    "# tfidf = models.TfidfModel(corpus, id2word=id2word)  # step 1 -- initialize a model\n",
    "# corpus = tfidf[corpus]\n",
    "# for doc in corpus:\n",
    "#     pprint(doc)\n",
    "#     break\n",
    "\n",
    "# # print('Number of unique tokens: %d' % len(id2word))\n",
    "# # print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topWords = {}\n",
    "# for doc in corpus:\n",
    "#     for iWord, tf_idf in doc:\n",
    "#         if iWord not in topWords:\n",
    "#             topWords[iWord] = 0\n",
    "\n",
    "#         if tf_idf > topWords[iWord]:\n",
    "#             topWords[iWord] = tf_idf\n",
    "# sum = 0\n",
    "# term = []\n",
    "# for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "# #     print(\"%2s: %-13s %s\" % (i, id2word[item[0]], item[1]))\n",
    "#     term.append(id2word[item[0]])\n",
    "#     sum += item[1]\n",
    "# #     if i == 100: break\n",
    "# # print (sum)\n",
    "# mean = sum/i\n",
    "# print ('Mean of tf-idf score: ' + str(mean))\n",
    "# # print (term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tf-idf\n",
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# # Create Dictionary\n",
    "# from gensim import models\n",
    "\n",
    "# low_value = 0.271734994034526\n",
    "# low_value_words = []\n",
    "\n",
    "# tfidf = models.TfidfModel(corpus, id2word=id2word)  # step 1 -- initialize a model\n",
    "# corpus = tfidf[corpus]\n",
    "# for doc in corpus:\n",
    "#     low_value_words += [id for id, value in tfidf[doc] if value < low_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word.filter_tokens(bad_ids=low_value_words)\n",
    "# print('Number of filtered unique tokens: %d' % len(id2word))\n",
    "# print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [id2word.doc2bow(doc) for doc in data_ready]\n",
    "# corpus = tfidf[corpus]\n",
    "# for doc in corpus:\n",
    "#     pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=10)\n",
    "\n",
    "# pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, num_topics, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=num_topics, \n",
    "                                                random_state=100,\n",
    "                                                chunksize=100,\n",
    "                                                passes=40,\n",
    "                                                iterations=1000,\n",
    "                                                alpha=a,\n",
    "                                                eta=1/num_topics,\n",
    "                                                eval_every = None)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=df['tokenz'], dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_1 = pd.DataFrame(model_results)\n",
    "model_results_1.to_csv('tuning/00_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_2 = pd.DataFrame(model_results)\n",
    "model_results_2.to_csv('tuning/11_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_3 = pd.DataFrame(model_results)\n",
    "model_results_3.to_csv('tuning/22_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_4 = pd.DataFrame(model_results)\n",
    "model_results_4.to_csv('tuning/33_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_5 = pd.DataFrame(model_results)\n",
    "model_results_5.to_csv('tuning/44_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_6 = pd.DataFrame(model_results)\n",
    "model_results_6.to_csv('tuning/55_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_7 = pd.DataFrame(model_results)\n",
    "model_results_7.to_csv('tuning/66_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n",
      "30\n",
      "0.2\n",
      "0.03333333333333333\n",
      "0.48142530321700044\n",
      "30\n",
      "0.5\n",
      "0.03333333333333333\n",
      "0.4640248946909255\n",
      "30\n",
      "1\n",
      "0.03333333333333333\n",
      "0.4685723248047855\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_8 = pd.DataFrame(model_results)\n",
    "model_results_8.to_csv('tuning/77_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.01\n",
      "0.2\n",
      "0.46215727299492704\n",
      "5\n",
      "0.05\n",
      "0.2\n",
      "0.475949740884384\n",
      "5\n",
      "0.1\n",
      "0.2\n",
      "0.4725864403698793\n",
      "5\n",
      "0.2\n",
      "0.2\n",
      "0.462187508057516\n",
      "5\n",
      "0.5\n",
      "0.2\n",
      "0.5139876480935635\n",
      "5\n",
      "1\n",
      "0.2\n",
      "0.5226020993962525\n",
      "10\n",
      "0.01\n",
      "0.1\n",
      "0.44537656739566345\n",
      "10\n",
      "0.05\n",
      "0.1\n",
      "0.44249188779580917\n",
      "10\n",
      "0.1\n",
      "0.1\n",
      "0.42562174744265013\n",
      "10\n",
      "0.2\n",
      "0.1\n",
      "0.43234725698831583\n",
      "10\n",
      "0.5\n",
      "0.1\n",
      "0.48420929799217927\n",
      "10\n",
      "1\n",
      "0.1\n",
      "0.45106929489707104\n",
      "20\n",
      "0.01\n",
      "0.05\n",
      "0.5002477441153024\n",
      "20\n",
      "0.05\n",
      "0.05\n",
      "0.515560077269345\n",
      "20\n",
      "0.1\n",
      "0.05\n",
      "0.5190624611176899\n",
      "20\n",
      "0.2\n",
      "0.05\n",
      "0.4884948191470663\n",
      "20\n",
      "0.5\n",
      "0.05\n",
      "0.48577223614656584\n",
      "20\n",
      "1\n",
      "0.05\n",
      "0.49059211602464076\n",
      "30\n",
      "0.01\n",
      "0.03333333333333333\n",
      "0.5148438144572455\n",
      "30\n",
      "0.05\n",
      "0.03333333333333333\n",
      "0.510141542831873\n",
      "30\n",
      "0.1\n",
      "0.03333333333333333\n",
      "0.5057254134472735\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_9 = pd.DataFrame(model_results)\n",
    "model_results_9.to_csv('tuning/88_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10\n",
    "\n",
    "topics_range = [5,10,20,30]\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = [.01,.05,.1,.2,.5,1]\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "# beta = [1/num_topics]\n",
    "# beta.append('symmetric')\n",
    "# beta.append('auto')\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# iterate through number of topics\n",
    "for num_topics in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for a in alpha:\n",
    "        # iterare through beta values\n",
    "#         for b in beta:\n",
    "        # get the coherence score for the given parameters\n",
    "        cv = compute_coherence_values(corpus=corpus, dictionary=id2word, \n",
    "                                      num_topics=num_topics, a=a, b=1/num_topics)\n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(num_topics)\n",
    "        model_results['Alpha'].append(a)\n",
    "        model_results['Beta'].append(1/num_topics)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        print (num_topics)\n",
    "        print (a)\n",
    "        print (1/num_topics)\n",
    "        print (cv)\n",
    "\n",
    "model_results_10 = pd.DataFrame(model_results)\n",
    "model_results_10.to_csv('tuning/99_lda_tuning_results.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.concat([model_results_1, model_results_2, model_results_3, model_results_4, model_results_5,\n",
    "                          model_results_6, model_results_7, model_results_8, model_results_9, model_results_10])\n",
    "model_results.to_csv(\"tuning/model_results.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.groupby(['Topics', 'Alpha'], as_index=False).mean()\n",
    "model_results = model_results.sort_values(by='Coherence', ascending=False)\n",
    "model_results.to_csv('2_lda_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors = pd.pivot_table(model_results,index=[\"Topics\"],columns=[\"Alpha\"],values=['Coherence'])\n",
    "# priors.columns = range(priors.shape[1])\n",
    "# priors.columns = ['.01','.05','.1','.2','.5','1']\n",
    "# df.head(1)\n",
    "# priors = priors.reset_index()\n",
    "# priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors.to_csv(\"siri_lda_tuning_results.csv\",index=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "  \n",
    "# # dummy data\n",
    "# x1 = priors['Topics']\n",
    "# A = priors['.01']\n",
    "# B = priors['.05']\n",
    "# C = priors['.1']\n",
    "# D = priors['.2']\n",
    "# E = priors['.5']\n",
    "# F = priors['1']\n",
    "\n",
    "# # creates two subplots\n",
    "# # fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (24, 12))\n",
    "\n",
    "# fig, ax = plt.subplots(2, 3, figsize = (24,12))\n",
    "\n",
    "# # Plot without grid\n",
    "# ax[0,0].plot(x1, A, label='0.01', color='tab:blue')\n",
    "# ax[0,1].plot(x1, B, label='0.05', color='tab:orange')\n",
    "# ax[0,2].plot(x1, C, label='0.1', color='tab:green')\n",
    "# ax[1,0].plot(x1, D, label='0.2', color='tab:red')\n",
    "# ax[1,1].plot(x1, E, label='0.5', color='tab:purple')\n",
    "# ax[1,2].plot(x1, F, label='1', color='tab:brown')\n",
    "\n",
    "# ax[0,0].set_xlim(xmin=9)\n",
    "# ax[0,0].set_title('siri, α=.01, Beta=1/K')\n",
    "# ax[0,0].set_xlabel('K')\n",
    "# ax[0,0].set_ylabel('Cv')\n",
    "\n",
    "# ax[0,1].set_xlim(xmin=9)\n",
    "# ax[0,1].set_title('siri, α=.05, Beta=1/K')\n",
    "# ax[0,1].set_xlabel('K')\n",
    "# ax[0,1].set_ylabel('Cv')\n",
    "\n",
    "# ax[0,2].set_xlim(xmin=9)\n",
    "# ax[0,2].set_title('siri, α=.1, Beta=1/K')\n",
    "# ax[0,2].set_xlabel('K')\n",
    "# ax[0,2].set_ylabel('Cv')\n",
    "\n",
    "# ax[1,0].set_xlim(xmin=9)\n",
    "# ax[1,0].set_title('siri, α=.2, Beta=1/K')\n",
    "# ax[1,0].set_xlabel('K')\n",
    "# ax[1,0].set_ylabel('Cv')\n",
    "\n",
    "# ax[1,1].set_xlim(xmin=9)\n",
    "# ax[1,1].set_title('siri, α=.5, Beta=1/K')\n",
    "# ax[1,1].set_xlabel('K')\n",
    "# ax[1,1].set_ylabel('Cv')\n",
    "\n",
    "# ax[1,2].set_xlim(xmin=9)\n",
    "# ax[1,2].set_title('siri, α=1, Beta=1/K')\n",
    "# ax[1,2].set_xlabel('K')\n",
    "# ax[1,2].set_ylabel('Cv')\n",
    "\n",
    "# # fig.tight_layout()\n",
    "# fig.set_facecolor(\"w\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_5 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                         id2word=id2word,\n",
    "                                         num_topics=5, \n",
    "                                         random_state=100,\n",
    "                                         chunksize=100,\n",
    "                                         passes=40,\n",
    "                                         iterations=1000,\n",
    "                                         alpha=0.10,\n",
    "                                         eta=1/5,\n",
    "                                         eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_5.show_topics(num_topics=5, num_words=30, log=False, formatted=False):\n",
    "    pprint('Topic {}: {}'.format(idx, ', '.join([w[0] for w in topic])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_5, corpus, id2word, R=30, sort_topics=False) #mds='tsne' \n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'LDAvis/lda_model_5.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_10 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                          id2word=id2word,\n",
    "                                          num_topics=10, \n",
    "                                          random_state=100,\n",
    "                                          chunksize=100,\n",
    "                                          passes=40,\n",
    "                                          iterations=1000,\n",
    "                                          alpha=0.05,\n",
    "                                          eta=1/10,\n",
    "                                          eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_10.show_topics(num_topics=10, num_words=30, log=False, formatted=False):\n",
    "    pprint('Topic {}: {}'.format(idx, ', '.join([w[0] for w in topic])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_10, corpus, id2word, R=30, sort_topics=False) #mds='tsne' \n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'LDAvis/lda_model_10.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_20 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                          id2word=id2word,\n",
    "                                          num_topics=20, \n",
    "                                          random_state=100,\n",
    "                                          chunksize=100,\n",
    "                                          passes=40,\n",
    "                                          iterations=1000,\n",
    "                                          alpha=0.01,\n",
    "                                          eta=1/20,\n",
    "                                          eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_20.show_topics(num_topics=20, num_words=30, log=False, formatted=False):\n",
    "    pprint('Topic {}: {}'.format(idx, ', '.join([w[0] for w in topic])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_20, corpus, id2word, R=30, sort_topics=False) #mds='tsne' \n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'LDAvis/lda_model_20.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K =  30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_30 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                          id2word=id2word,\n",
    "                                          num_topics=30, \n",
    "                                          random_state=100,\n",
    "                                          chunksize=100,\n",
    "                                          passes=40,\n",
    "                                          iterations=1000,\n",
    "                                          alpha=0.1,\n",
    "                                          eta=1/30,\n",
    "                                          eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_30.show_topics(num_topics=30, num_words=30, log=False, formatted=False):\n",
    "    pprint('Topic {}: {}'.format(idx, ', '.join([w[0] for w in topic])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_30, corpus, id2word, R=30, sort_topics=False) #mds='tsne' \n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'LDAvis/lda_model_30.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "lda_model_5.save(\"model/lda_model_5\")\n",
    "lda_model_10.save(\"model/lda_model_10\")\n",
    "lda_model_20.save(\"model/lda_model_20\")\n",
    "lda_model_30.save(\"model/lda_model_30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the dominant topic in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary.load(\"corpus_dict/dict\")\n",
    "corpus = corpora.MmCorpus(\"corpus_dict/corpus\")\n",
    "df=pd.read_csv('googlehome_merged.csv',encoding=\"utf-8\")\n",
    "lda = gensim.models.ldamodel.LdaModel.load(\"model/lda_model_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=df['content']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda, corpus=corpus, texts=df['content'])\n",
    "# df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Content']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"3_df_dominant_topic.csv\", encoding = 'utf-8',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Link\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.to_csv(\"4_sent_topics_sorteddf_mallet.csv\", encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
